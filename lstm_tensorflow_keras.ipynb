{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# \n",
    "import yfinance as yf\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_in_unison(a, b):\n",
    "    # shuffle two arrays in the same way\n",
    "    state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, split_by_date=True,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the dataset (both training & testing), default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        split_by_date (bool): whether we split the dataset into training/testing by date, setting it \n",
    "            to False will split datasets in a random way\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        df = si.get_data(ticker)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns, f\"'{col}' does not exist in the dataframe.\"\n",
    "    # add date as a column\n",
    "    if \"date\" not in df.columns:\n",
    "        df[\"date\"] = df.index\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "    for entry, target in zip(df[feature_columns + [\"date\"]].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 60 (that is 50+10) length\n",
    "    # this last_sequence will be used to predict future stock prices that are not available in the dataset\n",
    "    last_sequence = list([s[:len(feature_columns)] for s in sequences]) + list(last_sequence)\n",
    "    last_sequence = np.array(last_sequence).astype(np.float32)\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    if split_by_date:\n",
    "        # split the dataset into training & testing sets by date (not randomly splitting)\n",
    "        train_samples = int((1 - test_size) * len(X))\n",
    "        result[\"X_train\"] = X[:train_samples]\n",
    "        result[\"y_train\"] = y[:train_samples]\n",
    "        result[\"X_test\"]  = X[train_samples:]\n",
    "        result[\"y_test\"]  = y[train_samples:]\n",
    "        if shuffle:\n",
    "            # shuffle the datasets for training (if shuffle parameter is set)\n",
    "            shuffle_in_unison(result[\"X_train\"], result[\"y_train\"])\n",
    "            shuffle_in_unison(result[\"X_test\"], result[\"y_test\"])\n",
    "    else:    \n",
    "        # split the dataset randomly\n",
    "        result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, \n",
    "                                                                                test_size=test_size, shuffle=shuffle)\n",
    "    # get the list of test set dates\n",
    "    dates = result[\"X_test\"][:, -1, -1]\n",
    "    # retrieve test features from the original dataframe\n",
    "    result[\"test_df\"] = result[\"df\"].loc[dates]\n",
    "    # remove duplicated dates in the testing dataframe\n",
    "    result[\"test_df\"] = result[\"test_df\"][~result[\"test_df\"].index.duplicated(keep='first')]\n",
    "    # remove dates from the training/testing sets & convert to float32\n",
    "    result[\"X_train\"] = result[\"X_train\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    result[\"X_test\"] = result[\"X_test\"][:, :, :len(feature_columns)].astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from Yahoo Finance\n",
    "symbol = \"AAPL\"\n",
    "config = {\n",
    "    \"y_finance\": {\n",
    "        \"period\": \"10y\", # period to download the data from Yahoo Finance\n",
    "        \"symbol\": \"AAPL\", # TSLA - Tesla\n",
    "        \"key_adjusted_close\": \"adj close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 20,\n",
    "        \"train_split_size\": 0.80,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"show_plots\": True,\n",
    "        \"xticks_interval\": 90,\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since we are only using 1 feature, close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 32,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\", # \"cuda\" or \"cpu\"\n",
    "        \"batch_size\": 64,\n",
    "        \"num_epoch\": 100,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Get data of a period of recent years\n",
    "df = yf.download(symbol, period=config['y_finance']['period'])\n",
    "\n",
    "# Save the data to CSV file\n",
    "df.to_csv('data/' + symbol + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>18.035713</td>\n",
       "      <td>18.139999</td>\n",
       "      <td>17.981428</td>\n",
       "      <td>18.077499</td>\n",
       "      <td>15.777155</td>\n",
       "      <td>340687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>18.078571</td>\n",
       "      <td>18.123215</td>\n",
       "      <td>17.482143</td>\n",
       "      <td>17.665714</td>\n",
       "      <td>15.417765</td>\n",
       "      <td>743195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-09-11</td>\n",
       "      <td>16.678928</td>\n",
       "      <td>16.917500</td>\n",
       "      <td>16.600357</td>\n",
       "      <td>16.703930</td>\n",
       "      <td>14.578368</td>\n",
       "      <td>898696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-09-12</td>\n",
       "      <td>16.732143</td>\n",
       "      <td>16.978571</td>\n",
       "      <td>16.643213</td>\n",
       "      <td>16.881786</td>\n",
       "      <td>14.733592</td>\n",
       "      <td>404051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-09-13</td>\n",
       "      <td>16.762142</td>\n",
       "      <td>16.851070</td>\n",
       "      <td>16.596430</td>\n",
       "      <td>16.603571</td>\n",
       "      <td>14.490783</td>\n",
       "      <td>298835600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close     Volume\n",
       "0  2013-09-09  18.035713  18.139999  17.981428  18.077499  15.777155  340687200\n",
       "1  2013-09-10  18.078571  18.123215  17.482143  17.665714  15.417765  743195600\n",
       "2  2013-09-11  16.678928  16.917500  16.600357  16.703930  14.578368  898696400\n",
       "3  2013-09-12  16.732143  16.978571  16.643213  16.881786  14.733592  404051200\n",
       "4  2013-09-13  16.762142  16.851070  16.596430  16.603571  14.490783  298835600"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data from CSV file\n",
    "df = pd.read_csv('data/' + symbol + '.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names to lower case to process easier in latter parts\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-09-09</th>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>18.035713</td>\n",
       "      <td>18.139999</td>\n",
       "      <td>17.981428</td>\n",
       "      <td>18.077499</td>\n",
       "      <td>15.777155</td>\n",
       "      <td>340687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-10</th>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>18.078571</td>\n",
       "      <td>18.123215</td>\n",
       "      <td>17.482143</td>\n",
       "      <td>17.665714</td>\n",
       "      <td>15.417765</td>\n",
       "      <td>743195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-11</th>\n",
       "      <td>2013-09-11</td>\n",
       "      <td>16.678928</td>\n",
       "      <td>16.917500</td>\n",
       "      <td>16.600357</td>\n",
       "      <td>16.703930</td>\n",
       "      <td>14.578368</td>\n",
       "      <td>898696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-12</th>\n",
       "      <td>2013-09-12</td>\n",
       "      <td>16.732143</td>\n",
       "      <td>16.978571</td>\n",
       "      <td>16.643213</td>\n",
       "      <td>16.881786</td>\n",
       "      <td>14.733592</td>\n",
       "      <td>404051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-09-13</th>\n",
       "      <td>2013-09-13</td>\n",
       "      <td>16.762142</td>\n",
       "      <td>16.851070</td>\n",
       "      <td>16.596430</td>\n",
       "      <td>16.603571</td>\n",
       "      <td>14.490783</td>\n",
       "      <td>298835600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-08-31</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>187.839996</td>\n",
       "      <td>189.119995</td>\n",
       "      <td>187.479996</td>\n",
       "      <td>187.869995</td>\n",
       "      <td>187.869995</td>\n",
       "      <td>60794500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-01</th>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>189.490005</td>\n",
       "      <td>189.919998</td>\n",
       "      <td>188.279999</td>\n",
       "      <td>189.460007</td>\n",
       "      <td>189.460007</td>\n",
       "      <td>45732600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-05</th>\n",
       "      <td>2023-09-05</td>\n",
       "      <td>188.279999</td>\n",
       "      <td>189.979996</td>\n",
       "      <td>187.610001</td>\n",
       "      <td>189.699997</td>\n",
       "      <td>189.699997</td>\n",
       "      <td>45280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-06</th>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>188.399994</td>\n",
       "      <td>188.850006</td>\n",
       "      <td>181.470001</td>\n",
       "      <td>182.910004</td>\n",
       "      <td>182.910004</td>\n",
       "      <td>81755800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-09-07</th>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>175.179993</td>\n",
       "      <td>178.210007</td>\n",
       "      <td>173.539993</td>\n",
       "      <td>177.559998</td>\n",
       "      <td>177.559998</td>\n",
       "      <td>112380500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2517 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date        open  ...   adj close     volume\n",
       "date                                ...                       \n",
       "2013-09-09  2013-09-09   18.035713  ...   15.777155  340687200\n",
       "2013-09-10  2013-09-10   18.078571  ...   15.417765  743195600\n",
       "2013-09-11  2013-09-11   16.678928  ...   14.578368  898696400\n",
       "2013-09-12  2013-09-12   16.732143  ...   14.733592  404051200\n",
       "2013-09-13  2013-09-13   16.762142  ...   14.490783  298835600\n",
       "...                ...         ...  ...         ...        ...\n",
       "2023-08-31  2023-08-31  187.839996  ...  187.869995   60794500\n",
       "2023-09-01  2023-09-01  189.490005  ...  189.460007   45732600\n",
       "2023-09-05  2023-09-05  188.279999  ...  189.699997   45280000\n",
       "2023-09-06  2023-09-06  188.399994  ...  182.910004   81755800\n",
       "2023-09-07  2023-09-07  175.179993  ...  177.559998  112380500\n",
       "\n",
       "[2517 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set `date` as dataframe index\n",
    "df.set_index(pd.DatetimeIndex(df['date']), inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Window size or the sequence length\n",
    "N_STEPS = 50\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 15\n",
    "# whether to scale feature columns & output price as well\n",
    "SCALE = True\n",
    "scale_str = f\"sc-{int(SCALE)}\"\n",
    "# whether to shuffle the dataset\n",
    "SHUFFLE = True\n",
    "shuffle_str = f\"sh-{int(SHUFFLE)}\"\n",
    "# whether to split the training/testing set by date\n",
    "SPLIT_BY_DATE = False\n",
    "split_by_date_str = f\"sbd-{int(SPLIT_BY_DATE)}\"\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "### model parameters\n",
    "N_LAYERS = 2\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = False\n",
    "### training parameters\n",
    "# mean absolute error loss\n",
    "# LOSS = \"mae\"\n",
    "# huber loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 500\n",
    "# Amazon stock market\n",
    "ticker = \"AMZN\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save, making it as unique as possible based on parameters\n",
    "model_name = f\"{date_now}_{ticker}-{shuffle_str}-{scale_str}-{split_by_date_str}-\\\n",
    "{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, scale=SCALE, split_by_date=SPLIT_BY_DATE, \n",
    "                shuffle=SHUFFLE, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, \n",
    "                feature_columns=FEATURE_COLUMNS)\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df':                   open        high         low  ...    adjclose      volume  ticker\n",
       " 1997-05-15    0.121875    0.125000    0.096354  ...    0.097917  1443120000    AMZN\n",
       " 1997-05-16    0.098438    0.098958    0.085417  ...    0.086458   294000000    AMZN\n",
       " 1997-05-19    0.088021    0.088542    0.081250  ...    0.085417   122136000    AMZN\n",
       " 1997-05-20    0.086458    0.087500    0.081771  ...    0.081771   109344000    AMZN\n",
       " 1997-05-21    0.081771    0.082292    0.068750  ...    0.071354   377064000    AMZN\n",
       " ...                ...         ...         ...  ...         ...         ...     ...\n",
       " 2023-08-31  135.059998  138.789993  135.000000  ...  138.009995    58781300    AMZN\n",
       " 2023-09-01  139.460007  139.960007  136.880005  ...  138.119995    40948300    AMZN\n",
       " 2023-09-05  137.729996  137.800003  135.820007  ...  137.270004    40636700    AMZN\n",
       " 2023-09-06  136.320007  137.449997  134.610001  ...  135.360001    41785500    AMZN\n",
       " 2023-09-07  133.899994  138.029999  133.160004  ...  137.850006    48463200    AMZN\n",
       " \n",
       " [6622 rows x 7 columns],\n",
       " 'column_scaler': {'adjclose': MinMaxScaler(),\n",
       "  'volume': MinMaxScaler(),\n",
       "  'open': MinMaxScaler(),\n",
       "  'high': MinMaxScaler(),\n",
       "  'low': MinMaxScaler()},\n",
       " 'last_sequence': array([[0.6784972 , 0.01731053, 0.6679843 , 0.6751857 , 0.6761474 ],\n",
       "        [0.64965016, 0.04137021, 0.67835146, 0.6750266 , 0.6524969 ],\n",
       "        [0.6658431 , 0.02523801, 0.65697587, 0.6657998 , 0.6613185 ],\n",
       "        [0.66144633, 0.02002369, 0.66269386, 0.6667013 , 0.6663517 ],\n",
       "        [0.67828274, 0.02002754, 0.6623732 , 0.67189795, 0.66819173],\n",
       "        [0.67876536, 0.01965525, 0.68428314, 0.68054146, 0.6771216 ],\n",
       "        [0.67747843, 0.02054973, 0.6766948 , 0.6727994 , 0.6713849 ],\n",
       "        [0.68117815, 0.02441907, 0.6687324 , 0.6767235 , 0.67246723],\n",
       "        [0.67249185, 0.03584489, 0.68209213, 0.6778371 , 0.67777103],\n",
       "        [0.6740468 , 0.02272014, 0.6674499 , 0.67439026, 0.6734414 ],\n",
       "        [0.668953  , 0.0204126 , 0.6710303 , 0.6716329 , 0.6699236 ],\n",
       "        [0.69747835, 0.03881406, 0.6692668 , 0.6907227 , 0.6769051 ],\n",
       "        [0.6930816 , 0.02990659, 0.6895736 , 0.6934271 , 0.69389886],\n",
       "        [0.6823578 , 0.02419315, 0.6907492 , 0.69687396, 0.68751264],\n",
       "        [0.6922773 , 0.01784297, 0.68700856, 0.68945   , 0.6899481 ],\n",
       "        [0.6915266 , 0.02041828, 0.68866515, 0.69682086, 0.6947648 ],\n",
       "        [0.6854141 , 0.01493471, 0.68775666, 0.68504876, 0.68837863],\n",
       "        [0.6986044 , 0.0214588 , 0.6914974 , 0.6956012 , 0.6975249 ],\n",
       "        [0.69785374, 0.00891778, 0.6987117 , 0.6987829 , 0.7035864 ],\n",
       "        [0.69871163, 0.01259192, 0.6956122 , 0.69639665, 0.7012592 ],\n",
       "        [0.6878806 , 0.0148764 , 0.6849778 , 0.68223834, 0.6889739 ],\n",
       "        [0.6954945 , 0.01549696, 0.68679476, 0.6941165 , 0.6930871 ],\n",
       "        [0.6812854 , 0.025108  , 0.68935984, 0.68515486, 0.6811265 ],\n",
       "        [0.69013256, 0.01935994, 0.6823059 , 0.6877532 , 0.68886566],\n",
       "        [0.7009636 , 0.02132028, 0.6959862 , 0.6956543 , 0.69687545],\n",
       "        [0.7197303 , 0.02476209, 0.7159189 , 0.7137366 , 0.71787417],\n",
       "        [0.7217678 , 0.02149617, 0.7160258 , 0.724236  , 0.7251803 ],\n",
       "        [0.7157625 , 0.01863706, 0.7186978 , 0.7187742 , 0.72058016],\n",
       "        [0.7118483 , 0.02177592, 0.70881164, 0.7094414 , 0.7105138 ],\n",
       "        [0.7254139 , 0.02156497, 0.71244544, 0.72073627, 0.71689993],\n",
       "        [0.69645965, 0.02411192, 0.7160793 , 0.71437293, 0.6995815 ],\n",
       "        [0.6966741 , 0.05947545, 0.70149046, 0.69623756, 0.69465655],\n",
       "        [0.6902398 , 0.0172604 , 0.6959862 , 0.69777536, 0.6942777 ],\n",
       "        [0.69200927, 0.01420076, 0.69064236, 0.6867457 , 0.6952518 ],\n",
       "        [0.6867545 , 0.02126601, 0.6756795 , 0.6840943 , 0.6821548 ],\n",
       "        [0.6872907 , 0.02064035, 0.69967353, 0.70291907, 0.691247  ],\n",
       "        [0.7085239 , 0.01761012, 0.692673  , 0.70493406, 0.6995815 ],\n",
       "        [0.71640587, 0.01548386, 0.7114301 , 0.7094944 , 0.7160882 ],\n",
       "        [0.70573574, 0.01557872, 0.71330047, 0.70853996, 0.711975  ],\n",
       "        [0.68707633, 0.01987808, 0.6951312 , 0.6901924 , 0.6859973 ],\n",
       "        [0.69082963, 0.0379621 , 0.6808631 , 0.68812436, 0.6837784 ],\n",
       "        [0.7479876 , 0.06894835, 0.75343305, 0.76124924, 0.7536476 ],\n",
       "        [0.76219666, 0.02959742, 0.75305897, 0.7554692 , 0.7516451 ],\n",
       "        [0.7499715 , 0.0202069 , 0.7510817 , 0.74645454, 0.74877673],\n",
       "        [0.7387651 , 0.01939162, 0.7476082 , 0.74369717, 0.74163294],\n",
       "        [0.742572  , 0.02368232, 0.74279875, 0.7441744 , 0.7437436 ],\n",
       "        [0.74176776, 0.01593195, 0.7338744 , 0.7384474 , 0.74109167],\n",
       "        [0.75334954, 0.01801039, 0.73868394, 0.74512887, 0.7451507 ],\n",
       "        [0.73779994, 0.01590758, 0.7480357 , 0.74878776, 0.7423364 ],\n",
       "        [0.723859  , 0.01537523, 0.7327522 , 0.7275238 , 0.7303217 ],\n",
       "        [0.7180145 , 0.01859079, 0.7235073 , 0.7212665 , 0.722312  ],\n",
       "        [0.7139394 , 0.01864631, 0.7029867 , 0.710555  , 0.7094313 ],\n",
       "        [0.7217678 , 0.01526285, 0.71431583, 0.7164941 , 0.71787417],\n",
       "        [0.7194622 , 0.01116653, 0.7214766 , 0.7189333 , 0.72339433],\n",
       "        [0.72627187, 0.01591697, 0.7183771 , 0.72052413, 0.7206342 ],\n",
       "        [0.70654   , 0.01632398, 0.72853047, 0.7249254 , 0.7131115 ],\n",
       "        [0.7141539 , 0.01656531, 0.70752907, 0.7094944 , 0.7063465 ],\n",
       "        [0.7135105 , 0.01173148, 0.7145295 , 0.7099186 , 0.7132198 ],\n",
       "        [0.72300106, 0.01391638, 0.71239203, 0.7162289 , 0.7207966 ],\n",
       "        [0.723859  , 0.01270825, 0.720675  , 0.71909237, 0.72442263],\n",
       "        [0.73962295, 0.0236115 , 0.7213697 , 0.7355839 , 0.73026764],\n",
       "        [0.7402128 , 0.01502489, 0.7448829 , 0.7417882 , 0.7404423 ],\n",
       "        [0.7356552 , 0.01487486, 0.73563784, 0.7303342 , 0.7347055 ],\n",
       "        [0.7254139 , 0.01542801, 0.72810304, 0.72847825, 0.7281569 ],\n",
       "        [0.7387651 , 0.01864332, 0.7151708 , 0.73155385, 0.72030956]],\n",
       "       dtype=float32),\n",
       " 'X_train': array([[[0.01101448, 0.0604216 , 0.01111361, 0.01108859, 0.01111291],\n",
       "         [0.01085362, 0.24880491, 0.01112697, 0.01116018, 0.01070159],\n",
       "         [0.0092102 , 0.57810324, 0.00894934, 0.00913188, 0.00898869],\n",
       "         ...,\n",
       "         [0.00890189, 0.02663566, 0.00886918, 0.0088959 , 0.00894269],\n",
       "         [0.00890993, 0.05498739, 0.00881574, 0.00880046, 0.00875327],\n",
       "         [0.00882146, 0.03503303, 0.00880238, 0.00881901, 0.00888586]],\n",
       " \n",
       "        [[0.00549037, 0.12866567, 0.00475105, 0.00554855, 0.00478626],\n",
       "         [0.0047196 , 0.06335779, 0.00541903, 0.00538284, 0.00478626],\n",
       "         [0.00485364, 0.04436933, 0.00468425, 0.00505141, 0.00475243],\n",
       "         ...,\n",
       "         [0.00206545, 0.08008994, 0.00238437, 0.002408  , 0.00194495],\n",
       "         [0.00193945, 0.04216502, 0.00195152, 0.00204211, 0.00194495],\n",
       "         [0.00187778, 0.05027734, 0.00191411, 0.00200234, 0.0018367 ]],\n",
       " \n",
       "        [[0.5471304 , 0.05284822, 0.5402654 , 0.54267013, 0.5304558 ],\n",
       "         [0.5360849 , 0.03184371, 0.54491454, 0.54404885, 0.53424424],\n",
       "         [0.5263798 , 0.02642404, 0.54106694, 0.53932935, 0.5276957 ],\n",
       "         ...,\n",
       "         [0.55050844, 0.01452953, 0.55089974, 0.54966974, 0.549452  ],\n",
       "         [0.54814917, 0.01446741, 0.55512136, 0.552162  , 0.54907316],\n",
       "         [0.558873  , 0.02342737, 0.5424564 , 0.5570406 , 0.5483696 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.00837106, 0.05629899, 0.00851381, 0.00851941, 0.00843666],\n",
       "         [0.0084488 , 0.06010381, 0.00833746, 0.0084054 , 0.00837713],\n",
       "         [0.00862039, 0.05298916, 0.00846839, 0.00855388, 0.00847455],\n",
       "         ...,\n",
       "         [0.01035228, 0.07382658, 0.01014637, 0.01027197, 0.01026051],\n",
       "         [0.01015121, 0.05777527, 0.01036547, 0.01039128, 0.01016039],\n",
       "         [0.01019411, 0.06772597, 0.00998605, 0.01014735, 0.00998991]],\n",
       " \n",
       "        [[0.05154248, 0.02463069, 0.05100039, 0.05116938, 0.05121057],\n",
       "         [0.05011353, 0.04321276, 0.05071983, 0.05084326, 0.050415  ],\n",
       "         [0.04940844, 0.03109339, 0.04985947, 0.0499524 , 0.04975474],\n",
       "         ...,\n",
       "         [0.05460949, 0.09633867, 0.05218139, 0.05399309, 0.05118351],\n",
       "         [0.05167116, 0.07964408, 0.0532662 , 0.05327987, 0.0520332 ],\n",
       "         [0.05280521, 0.06658866, 0.05226422, 0.05286891, 0.05142705]],\n",
       " \n",
       "        [[0.07864425, 0.03514185, 0.07910924, 0.07955497, 0.07899318],\n",
       "         [0.07825015, 0.04864602, 0.07762631, 0.07807285, 0.07717203],\n",
       "         [0.07655579, 0.03786522, 0.0781794 , 0.07809672, 0.07725862],\n",
       "         ...,\n",
       "         [0.10019109, 0.01687275, 0.10064244, 0.10083753, 0.10110398],\n",
       "         [0.09991763, 0.01676586, 0.09955227, 0.09910618, 0.10038149],\n",
       "         [0.09907849, 0.02833439, 0.09986757, 0.10050081, 0.09984569]]],\n",
       "       dtype=float32),\n",
       " 'X_test': array([[[0.7271887 , 0.02620905, 0.7169076 , 0.7194636 , 0.721879  ],\n",
       "         [0.7407517 , 0.03605959, 0.7280015 , 0.7375221 , 0.73514926],\n",
       "         [0.7327061 , 0.03889948, 0.7424246 , 0.74093974, 0.7359503 ],\n",
       "         ...,\n",
       "         [0.9111505 , 0.03637834, 0.92145824, 0.9151348 , 0.9137351 ],\n",
       "         [0.9116331 , 0.02320641, 0.9142306 , 0.9099302 , 0.91603523],\n",
       "         [0.9248127 , 0.03561854, 0.9104872 , 0.9262706 , 0.92104137]],\n",
       " \n",
       "        [[0.57214373, 0.04014561, 0.5535476 , 0.57324046, 0.5606008 ],\n",
       "         [0.59521335, 0.04008879, 0.57660383, 0.5970524 , 0.5811854 ],\n",
       "         [0.617031  , 0.04040562, 0.6064227 , 0.61042327, 0.60918987],\n",
       "         ...,\n",
       "         [0.7477195 , 0.0298934 , 0.7275152 , 0.7445986 , 0.7359503 ],\n",
       "         [0.7640733 , 0.02929499, 0.75086796, 0.760878  , 0.75489235],\n",
       "         [0.75458276, 0.01971404, 0.74830294, 0.7571661 , 0.75516295]],\n",
       " \n",
       "        [[0.01000108, 0.11476956, 0.00995933, 0.00997236, 0.00994121],\n",
       "         [0.0100547 , 0.05615454, 0.00993529, 0.00999357, 0.00999803],\n",
       "         [0.00991261, 0.05905607, 0.00997002, 0.00990077, 0.00981943],\n",
       "         ...,\n",
       "         [0.00976515, 0.04542382, 0.0098578 , 0.00992464, 0.00984379],\n",
       "         [0.00983486, 0.07804067, 0.00971351, 0.00974965, 0.00972743],\n",
       "         [0.00975443, 0.06527705, 0.00980436, 0.00976821, 0.00980591]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.00707079, 0.09779184, 0.00699615, 0.00705055, 0.00707283],\n",
       "         [0.00691262, 0.05729666, 0.00699615, 0.00702669, 0.00697812],\n",
       "         [0.00660431, 0.08404692, 0.00672361, 0.00670587, 0.00666964],\n",
       "         ...,\n",
       "         [0.00911368, 0.08518904, 0.00897606, 0.00905499, 0.00903469],\n",
       "         [0.00914853, 0.0645837 , 0.00901881, 0.00911862, 0.00901034],\n",
       "         [0.00856677, 0.10490649, 0.00925394, 0.00922998, 0.00864232]],\n",
       " \n",
       "        [[0.00453528, 0.26557463, 0.00452838, 0.00465923, 0.00452129],\n",
       "         [0.00437051, 0.21738026, 0.00464807, 0.00461504, 0.00415485],\n",
       "         [0.00451852, 0.1744612 , 0.00427233, 0.00458742, 0.00429297],\n",
       "         ...,\n",
       "         [0.00370028, 0.19490957, 0.0036433 , 0.00367602, 0.00342198],\n",
       "         [0.00378964, 0.20540822, 0.00394946, 0.00407925, 0.00383915],\n",
       "         [0.00370865, 0.09820881, 0.00371289, 0.00383068, 0.00373204]],\n",
       " \n",
       "        [[0.3131554 , 0.02119759, 0.31554154, 0.31353855, 0.31557155],\n",
       "         [0.318394  , 0.02125633, 0.31277606, 0.31512937, 0.31638607],\n",
       "         [0.32246637, 0.02524605, 0.31713134, 0.31923637, 0.32120004],\n",
       "         ...,\n",
       "         [0.42291912, 0.03897363, 0.4173827 , 0.4182518 , 0.42153352],\n",
       "         [0.42814696, 0.04513588, 0.42515802, 0.4252488 , 0.4290075 ],\n",
       "         [0.42540967, 0.05821055, 0.43139967, 0.4284861 , 0.42665595]]],\n",
       "       dtype=float32),\n",
       " 'y_train': array([0.0087303 , 0.0037625 , 0.59045466, ..., 0.00986703, 0.0566068 ,\n",
       "        0.10312137]),\n",
       " 'y_test': array([0.8384939 , 0.70069552, 0.0108    , ..., 0.00935765, 0.00525858,\n",
       "        0.37779327]),\n",
       " 'test_df':                   open        high         low  ...    adjclose     volume  ticker\n",
       " 2020-08-31  170.449493  174.750000  170.250000  ...  172.548004   83718000    AMZN\n",
       " 2022-08-05  140.100006  142.860001  139.600006  ...  140.800003   50686900    AMZN\n",
       " 2007-03-15    1.905000    1.914500    1.877500  ...    1.889000  145314000    AMZN\n",
       " 2023-05-10  108.099998  110.669998  108.050003  ...  110.190002   78627600    AMZN\n",
       " 2011-10-05   10.626500   11.008500   10.424000  ...   10.975000  130164000    AMZN\n",
       " ...                ...         ...         ...  ...         ...        ...     ...\n",
       " 2000-04-11    3.125000    3.300000    3.062500  ...    3.168750   94132000    AMZN\n",
       " 2003-11-19    2.435500    2.492500    2.383500  ...    2.476500  281580000    AMZN\n",
       " 2003-06-06    1.802000    1.813000    1.662500  ...    1.667500  227618000    AMZN\n",
       " 1998-10-13    0.765104    0.794792    0.755208  ...    0.761458  213708000    AMZN\n",
       " 2018-03-13   80.797997   80.876999   78.900497  ...   79.408997  130638000    AMZN\n",
       " \n",
       " [1312 rows x 7 columns]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
